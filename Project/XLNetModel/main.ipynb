{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet version 1\n",
    "\n",
    "Hate speech identification project, D7047E <br>\n",
    "Binary text classification task using pretrained XLNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install sentencepiece\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install accelerate\n",
    "%pip install imbalanced-learn\n",
    "\"\"\"\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torchvision\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import XLNetConfig, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "# Misc\n",
    "from tqdm import tqdm, trange\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set up CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and vars\n",
    "\n",
    "# aux files\n",
    "path_tr = r\"..\\\\datasets\\\\OLID\\\\OLID_Tain.txt\" \n",
    "path_spm = r\"data\\\\proj_xlnet\" # +.model / +.vocab\n",
    "path_output = r\"trained\\\\\"\n",
    "\n",
    "# Constants\n",
    "GLOBAL_SEED = 1337\n",
    "TOKENIZER_MAX_LENGTH = 128 #100 works with 1 epoch \n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "tokenizer_config = {\n",
    "    \"padding\": \"max_length\", \n",
    "    \"truncation\": \"longest_first\",\n",
    "    \"max_length\": TOKENIZER_MAX_LENGTH,\n",
    "    #\"add_special_tokens\": True,\n",
    "    #\"return_tensors\": \"pt\",\n",
    "    #\"return_token_type_ids\": False, \n",
    "    #\"return_attention_mask\": True, \n",
    "    #\"pad_to_max_length\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\n",
    "    'xlnet-base-cased',\n",
    "    #additional_special_tokens = [\"@USER\"], NOTE THIS LINE RUINS EVERYTHING\n",
    "    # ^ this note was written at 1:30 am on a tuesday\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def tokenize_examples(examples):\n",
    "    return tokenizer(examples[\"text\"], **tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13240/13240 [00:00<00:00, 2648065.66it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ebd3f428d241cd99a78ecc0c79e590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 13240\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load and split dataset\n",
    "olid_dataset = pd.read_csv(path_tr, sep=\"\\t\", names=[\"id\",\"text\",\"label\",\"other_1\",\"other_2\"])\n",
    "olid_dataset = olid_dataset.drop(axis=0, index=0) # Remove column names\n",
    "olid_dataset = olid_dataset.drop(axis=1, labels=[\"id\", \"other_1\", \"other_2\"]).to_dict()\n",
    "for i in trange(len(olid_dataset[\"label\"])):\n",
    "    olid_dataset[\"label\"][i+1] = 1 if olid_dataset[\"label\"][i+1] == \"OFF\" else 0 \n",
    "\n",
    "olid_dataset = {\n",
    "    \"label\": [y for y in olid_dataset[\"label\"].values()],\n",
    "    \"text\": [x for x in olid_dataset[\"text\"].values()]     \n",
    "}\n",
    "\n",
    "olid_dataset = Dataset.from_dict(olid_dataset)\n",
    "olid_dataset = olid_dataset.map(tokenize_examples, batched=True)\n",
    "print(olid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 9268\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1324\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2648\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def _get_split(s):\n",
    "    return [int(x * olid_dataset.num_rows) for x in s]\n",
    "    \n",
    "split = _get_split([0.7, 0.1, 0.2])\n",
    "olid_train = olid_dataset.shuffle(seed=GLOBAL_SEED).select(range(split[0]))\n",
    "olid_val = olid_dataset.shuffle(seed=GLOBAL_SEED).select(range(split[1]))\n",
    "olid_test = olid_dataset.shuffle(seed=GLOBAL_SEED).select(range(split[2]))\n",
    "print(olid_train)\n",
    "print(olid_val)\n",
    "print(olid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12412, 128) (9268, 128)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m X_res, y_res \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_res\u001b[38;5;241m.\u001b[39mshape, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 13\u001b[0m X_res \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m olid_train_os \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_res})\n\u001b[0;32m     16\u001b[0m olid_train_os \u001b[38;5;241m=\u001b[39m olid_train_os\u001b[38;5;241m.\u001b[39mmap(tokenize_examples, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m X_res, y_res \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_res\u001b[38;5;241m.\u001b[39mshape, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 13\u001b[0m X_res \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_res))]\n\u001b[0;32m     15\u001b[0m olid_train_os \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_res})\n\u001b[0;32m     16\u001b[0m olid_train_os \u001b[38;5;241m=\u001b[39m olid_train_os\u001b[38;5;241m.\u001b[39mmap(tokenize_examples, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3809\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3812\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3813\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3814\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3816\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:275\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m \u001b[43m_get_frameworks_and_test_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:119\u001b[0m, in \u001b[0;36m_get_frameworks_and_test_func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    117\u001b[0m     frameworks\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m frameworks\u001b[38;5;241m.\u001b[39mextend([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m framework_to_test \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [preferred_framework, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework_to_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mframeworks\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:119\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    117\u001b[0m     frameworks\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m frameworks\u001b[38;5;241m.\u001b[39mextend([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m framework_to_test \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [preferred_framework, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {f: framework_to_test[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m frameworks}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prevent overfitting by oversampling class 1\n",
    "sm = SMOTE()\n",
    "\n",
    "X = olid_train[\"input_ids\"]\n",
    "y = olid_train[\"label\"]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print(X_res.shape, X.shape)\n",
    "\n",
    "X_res = [tokenizer.decode(X_res[i], skip_special_tokens=True) for i in range(len(X_res))]\n",
    "\n",
    "olid_train_os = Dataset.from_dict({\"text\": X_res, \"label\":y_res})\n",
    "olid_train_os = olid_train_os.map(tokenize_examples, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainLabelDistrib(loader, label=\"label\"):\n",
    "    labels_dist = {}\n",
    "    for e in loader:\n",
    "        lab = e[label]\n",
    "        if lab in labels_dist:\n",
    "            labels_dist[lab] += 1\n",
    "        else:\n",
    "            labels_dist[lab] = 0\n",
    "    print(labels_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getTrainLabelDistrib(olid_test)\n",
    "#getTrainLabelDistrib(olid_train)\n",
    "#getTrainLabelDistrib(olid_val)\n",
    "getTrainLabelDistrib(olid_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "# docs: https://huggingface.co/docs/transformers/model_doc/xlnet\n",
    "\n",
    "xlnet_model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\", \n",
    "    num_labels=2\n",
    ")\n",
    "xlnet_model.to(device)\n",
    "\n",
    "print(xlnet_model.config)\n",
    "print(next(xlnet_model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "# https://huggingface.co/docs/transformers/training\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_model_metrics(eval_pred):\n",
    "    global metric\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "device_batch_size = 32\n",
    "grad_steps = 8\n",
    "calc_steps = len(olid_train_os) // (device_batch_size * grad_steps * 5) \n",
    "\n",
    "\n",
    "tr_args = TrainingArguments(\n",
    "    do_train=True, do_eval=True, evaluation_strategy=\"epoch\", output_dir=path_output,\n",
    "    \n",
    "    logging_steps=calc_steps, # For larger size dataset\n",
    "    eval_accumulation_steps=10, # To prevent cuda OOM after training\n",
    "    per_device_eval_batch_size=device_batch_size,\n",
    "    per_device_train_batch_size=device_batch_size,\n",
    "    bf16=True, \n",
    "    # dataloader_pin_memory=True, dataloader_num_workers=8, # Load data on GPU\n",
    "    #fp16=True,\n",
    "\n",
    "    num_train_epochs = 10, \n",
    "    gradient_accumulation_steps=grad_steps,\n",
    "    weight_decay=0.1,\n",
    "    save_strategy=\"no\",\n",
    "    # use_cpu=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=xlnet_model,\n",
    "    args=tr_args,\n",
    "    train_dataset=olid_train,\n",
    "    eval_dataset=olid_val,\n",
    "    compute_metrics=compute_model_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(olid_test)\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape) \n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "labels_t = {\"0\":0, \"1\":0}\n",
    "labels_p = {\"0\":0, \"1\":0}\n",
    "for i in range(len(predictions.predictions)):\n",
    "    #print(predictions.label_ids[i], preds[i], predictions.predictions[i]) # Debug overfitting\n",
    "    labels_t[str(predictions.label_ids[i])] += 1\n",
    "    labels_p[str(preds[i])] += 1\n",
    "print(f\"True labels {labels_t}\")\n",
    "print(f\"Predicted labels {labels_p}\")\n",
    "\n",
    "loss_his = {\"tr_loss\":[], \"val_loss\":[]}\n",
    "\n",
    "for e in trainer.state.log_history:\n",
    "    if \"loss\" in e:\n",
    "        loss_his[\"tr_loss\"].append(e[\"loss\"])\n",
    "    elif \"eval_loss\" in e:\n",
    "        loss_his[\"val_loss\"].append(e[\"eval_loss\"])\n",
    "\n",
    "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\", \"BucketHeadP65/confusion_matrix\"])\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_his[\"tr_loss\"], label=\"tr_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0,2])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_his[\"val_loss\"], label=\"tr_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
