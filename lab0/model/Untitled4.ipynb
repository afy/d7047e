{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712328701571,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"GlNMTer-bun6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","from torch.utils.data import TensorDataset\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import copy"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1712325514789,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"1uLhPYWflR7b"},"outputs":[],"source":["# Train augmentations\n","transforms = transforms.Compose([\n","    # Add training augmentations here, remember: we do not want to transform the validation images.\n","    # For information about augmentation see: https://pytorch.org/vision/stable/transforms.html\n","    transforms.ToTensor(),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","\n","])"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2354,"status":"ok","timestamp":1712319947404,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"YrFvDCNHcFNR","outputId":"24bfab3a-b572-4dab-fad3-e4945cbd86df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:16<00:00, 10161899.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms)\n","\n","cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nB2CCN89f2Ju"},"outputs":[],"source":["# Spliting dataset\n","train_size= int(0.8 * len(cifar_trainset))\n","val_size = len(cifar_trainset) - train_size\n","train_dataset, val_dataset = random_split(cifar_trainset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(cifar_testset, batch_size=32, shuffle=False)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"jjbY0hgCchX_"},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n","10000\n","torch.Size([32, 3, 32, 32])\n"]}],"source":["print(val_size)\n","print(len(cifar_testset))\n","images, labels = next(iter(train_loader))\n","print(images.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RJxSVXXmhqq7"},"outputs":[],"source":["input_channel = 3\n","output_channel = 10\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv = nn.Conv2d(input_channel,output_channel, kernel_size=3, stride=1, padding=1)\n","        self.relu = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n","        self.maxpool = nn.AvgPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(16*16*output_channel, 10)  # Adjust the input size here\n","        self.dropout = nn.Dropout(0.3, inplace=False)\n","        self.fc2 = nn.Linear(10, 10)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = x.view(x.size(0), -1)  # Flatten the output before passing to fully connected layer\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","\n","# Instantiate the CNN, loss function, and optimizer\n","net = CNN()\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=1e-3, weight_decay=1e-2)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124163,"status":"ok","timestamp":1712320894582,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"vFe8MYXplI9Q","outputId":"05e2eb16-6b60-4d43-cdb0-fdb3c9f548ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n"]}],"source":["epochs = 5\n","\n","\n","for epoch in range(epochs):\n","\n","    print(epoch+1)\n","    # Training phase\n","    for batch_nr, (images, labels) in enumerate(train_loader):\n","        images = images.float()\n","        labels = labels.long()  # Ensure labels are long integers for CrossEntropyLoss\n","\n","        # Forward pass\n","        outputs = net(images)  # Do not apply sigmoid here\n","\n","        # Compute loss\n","        loss = loss_function(outputs, labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p119k-FtlIoF"},"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"E6Gxmpx8kXbY"},"outputs":[],"source":["def validation(model, device, val_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():  # No gradients needed for testing\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 validation images: %d %%' % (100 * correct / total))\n","\n","def test(model, device, test_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():  # No gradients needed for testing\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9358,"status":"ok","timestamp":1712321249217,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"IPDBXwDoSRA6","outputId":"03afb412-877a-4d84-a02d-fa578e2f5b86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the 10000 validation images: 29 %\n","Accuracy of the network on the 10000 test images: 29 %\n"]}],"source":["validate_model = validation(net, 'cpu', val_loader)\n","test_model = test(net,'cpu', test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1712328704981,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"-PjAURyYhKWa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":394,"status":"ok","timestamp":1712328708322,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"zFef44mRhdsW"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":283,"status":"error","timestamp":1712328711988,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"ByuF2j0_hu9C","outputId":"406f2104-c0ba-4e39-bb7e-83ade0f361b0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384981,"status":"ok","timestamp":1712329390764,"user":{"displayName":"Francesco Gigante","userId":"14871258670871532492"},"user_tz":-120},"id":"SxYgUC6rwFLP","outputId":"69de3392-07ad-4878-9996-e9865038bb5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 13941601.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 16825962.89it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Batch 100, Loss: 0.6880\n","Epoch 1, Batch 200, Loss: 0.1760\n","Epoch 1, Batch 300, Loss: 0.1279\n","Epoch 1, Batch 400, Loss: 0.0960\n","Epoch 1, Batch 500, Loss: 0.0896\n","Epoch 1, Batch 600, Loss: 0.0718\n","Epoch 1, Batch 700, Loss: 0.0758\n","Epoch 1, Batch 800, Loss: 0.0766\n","Epoch 1, Batch 900, Loss: 0.0628\n","Epoch 2, Batch 100, Loss: 0.0549\n","Epoch 2, Batch 200, Loss: 0.0580\n","Epoch 2, Batch 300, Loss: 0.0454\n","Epoch 2, Batch 400, Loss: 0.0507\n","Epoch 2, Batch 500, Loss: 0.0458\n","Epoch 2, Batch 600, Loss: 0.0416\n","Epoch 2, Batch 700, Loss: 0.0446\n","Epoch 2, Batch 800, Loss: 0.0444\n","Epoch 2, Batch 900, Loss: 0.0397\n","Epoch 3, Batch 100, Loss: 0.0225\n","Epoch 3, Batch 200, Loss: 0.0309\n","Epoch 3, Batch 300, Loss: 0.0306\n","Epoch 3, Batch 400, Loss: 0.0395\n","Epoch 3, Batch 500, Loss: 0.0343\n","Epoch 3, Batch 600, Loss: 0.0366\n","Epoch 3, Batch 700, Loss: 0.0311\n","Epoch 3, Batch 800, Loss: 0.0360\n","Epoch 3, Batch 900, Loss: 0.0367\n","Epoch 4, Batch 100, Loss: 0.0212\n","Epoch 4, Batch 200, Loss: 0.0226\n","Epoch 4, Batch 300, Loss: 0.0279\n","Epoch 4, Batch 400, Loss: 0.0227\n","Epoch 4, Batch 500, Loss: 0.0247\n","Epoch 4, Batch 600, Loss: 0.0185\n","Epoch 4, Batch 700, Loss: 0.0219\n","Epoch 4, Batch 800, Loss: 0.0298\n","Epoch 4, Batch 900, Loss: 0.0269\n","Epoch 5, Batch 100, Loss: 0.0172\n","Epoch 5, Batch 200, Loss: 0.0141\n","Epoch 5, Batch 300, Loss: 0.0119\n","Epoch 5, Batch 400, Loss: 0.0180\n","Epoch 5, Batch 500, Loss: 0.0208\n","Epoch 5, Batch 600, Loss: 0.0137\n","Epoch 5, Batch 700, Loss: 0.0187\n","Epoch 5, Batch 800, Loss: 0.0207\n","Epoch 5, Batch 900, Loss: 0.0195\n","Accuracy of the network on the 10000 test images: 99 %\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Define transformations for the dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Convert PIL image to tensor\n","    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values\n","])\n","\n","# Download and load the MNIST dataset\n","train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n","test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n","\n","# Define data loaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","# Define the CNN model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Instantiate the model, loss function, and optimizer\n","model = CNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training the model\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criterion(outputs, target)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if batch_idx % 100 == 99:\n","            print(f'Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {running_loss / 100:.4f}')\n","            running_loss = 0.0\n","\n","\n","tested_model = test(model,'cpu',test_loader)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
