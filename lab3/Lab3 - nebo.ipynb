{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms as tf\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Insperation guides:\n",
    "https://medium.com/@raman.shinde15/image-captioning-with-flickr8k-dataset-bleu-4bcba0b52926\n",
    "https://thepythoncode.com/article/image-captioning-with-pytorch-and-transformers-in-python\n",
    "\n",
    "Overall comments:\n",
    "Image captioning uses one to many RNN's.\n",
    "\n",
    "About Flickr 8k dataset\n",
    "Images: Contains a total of 8092 jpg format with different shapes and sizes.\n",
    "        6000 for train, 1000 for test, 1000 for development\n",
    "Captions.txt: Contains 5 captions for each image, total of 40460 captions.\n",
    "\n",
    "Size of training vocabulary: 7371\n",
    "\n",
    "Architecture\n",
    "We will use CNN + LSTM with attention.\n",
    "CNN: To extract features from the image.\n",
    "LSTM: To generate a description from the extracted information of the image\n",
    "\n",
    "Note:\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel\n",
    "RGB images of shape (3 x H x W), where H and W are expected to be at least 299. The images have to be loaded in\n",
    "to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Prepare the Dataset\n",
    "1.1 Dataset Organization: Ensure your dataset is properly organized. The Flickr8k dataset should include:\n",
    "An images folder containing all JPEG images.\n",
    "A captions.txt file where each line corresponds to an image and its caption in the format image_filename,caption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38009\n",
      "Batch Images shape torch.Size([32, 3, 224, 224])\n",
      "Batch captions shape: 32\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"flickr 8k/Images\"\n",
    "images = []\n",
    "for filename in os.listdir(image_folder):\n",
    "    img = mpimg.imread(os.path.join(image_folder, filename))\n",
    "    if img is not None:\n",
    "        images.append(img)\n",
    "\n",
    "\n",
    "print(f'There are {len(images)} images in this folder')\n",
    "\n",
    "captions = pd.read_csv('flickr 8k/captions.txt')\n",
    "print(f'There are {len(captions)} image to captions')\n",
    "print(f'How the data is structured:\\n {captions.head(7)}')\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for index, row in captions.iterrows():\n",
    "    img_path = os.path.join(image_folder, row['image'])  # Include the filename\n",
    "    if os.path.exists(img_path):\n",
    "        with Image.open(img_path) as img:\n",
    "            if img_path not in dataset:\n",
    "                dataset[img_path] = {'image': img, 'captions': []}\n",
    "            dataset[img_path]['captions'].append(row['caption'])\n",
    "\n",
    "print(f'Loaded {type(dataset)} images with captions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Data Loading and Preprocessing:\n",
    "Load the images and corresponding captions into your program.\n",
    "Normalize the images (resize if necessary and scale pixel values).\n",
    "Preprocess the captions by tokenizing (splitting text into words), converting to lowercase, and possibly \n",
    "removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tf.Compose([\n",
    "    tf.Resize(299),\n",
    "    tf.CenterCrop(299),\n",
    "    tf.ToTensor(),\n",
    "    tf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Feature Extraction with CNN\n",
    "2.1 Choose a Pre-trained CNN: \n",
    "Select a pre-trained model like VGG16, ResNet, or InceptionV3, \n",
    "which are commonly used for feature extraction in image captioning tasks.\n",
    "2.2 Modify the CNN: \n",
    "Since you need to extract features rather than perform classification, \n",
    "modify the last layer of the CNN to output features directly, rather than class probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Sequence Model with LSTM\n",
    "'''\n",
    "3.1 Prepare Text Data:\n",
    "Tokenize captions to convert each caption into a sequence of tokens.\n",
    "Pad the sequences to ensure they have the same length for batch processing.\n",
    "Create a vocabulary index (word-to-index and index-to-word mappings).\n",
    "3.2 Design the LSTM Model:\n",
    "The LSTM should take the sequence of tokens as input and learn to predict the next token in the sequence.\n",
    "Integrate embedding layers to convert tokens into dense vectors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Combine Image and Text Data with Attention\n",
    "'''\n",
    "4.1 Implement Attention Mechanism:\n",
    "Use an attention mechanism to allow the LSTM to focus on different parts of the image at different points in the\n",
    "sequence generation.\n",
    "This typically involves calculating alignment scores between the LSTM hidden state and the image features.\n",
    "4.2 Integrate the Models:\n",
    "The final model will concatenate the features from the CNN with the output from the attention mechanism.\n",
    "This concatenated output is then fed into the LSTM to generate the next word in the caption.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the Network\n",
    "'''\n",
    "5.1 Setup Training:\n",
    "Define the loss function, typically categorical crossentropy, since this is a multi-class classification problem \n",
    "(predicting the next word).\n",
    "Choose an optimizer like Adam for efficient training.\n",
    "Prepare your data batches and split the data into training and validation sets.\n",
    "5.2 Training Loop:\n",
    "For each epoch, train the combined model on the training set and evaluate its performance on the validation set.\n",
    "Save checkpoints and possibly adjust learning rates based on performance improvements.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluate with BLEU Score\n",
    "'''\n",
    "6.1 Generate Captions:\n",
    "Use the trained model to generate captions for images in the test set by predicting one word at a time and feeding\n",
    "it back into the model as input for the next word.\n",
    "6.2 Calculate BLEU Score:\n",
    "Use the BLEU score to evaluate the quality of the generated captions compared to the reference captions.\n",
    "BLEU scores provide a quantitative measure of how the generated captions match the reference captions in terms\n",
    "of precision of word use at various n-gram levels.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder, Recurrent Neural Network, LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam search, to transform the Decoder's output into a score for each word in the vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
