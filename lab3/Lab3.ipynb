{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38009\n",
      "Batch Images shape torch.Size([32, 3, 224, 224])\n",
      "Batch captions shape: 32\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Because of issues importing flickr 8k dataset directly as a folder.\n",
    "1. Add flickr 8k folder to your google drive.\n",
    "2. Add a BASE_DIR path by commenting out the exsiting BASE_DIR \n",
    "\n",
    "Or if it works for you to add the dataset folder in google colab, go ahead.\n",
    "'''\n",
    "\n",
    "# flickr8k dataset (images and caption.txt)\n",
    "BASE_DIR = '/content/drive/MyDrive/LTU/Deep Learning/Lab 3/flickr 8k' # nebos path\n",
    "image_folder = os.path.join(BASE_DIR, 'images')\n",
    "caption_file = os.path.join(BASE_DIR, 'captions.txt')\n",
    "\n",
    "\n",
    "# transform images to fit ImageNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except IOError:\n",
    "          print(f'Error opening image {img_name}, skipping')\n",
    "          return None\n",
    "\n",
    "        return image\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize dataset\n",
    "flickr_dataset = Flickr8kDataset(image_folder=image_folder, transform=transform)\n",
    "\n",
    "# Load and organize captions\n",
    "def load_captions(filename):\n",
    "    captions_dict = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            image_id, caption = row[0], row[1]\n",
    "            if image_id in captions_dict:\n",
    "                captions_dict[image_id].append(caption)\n",
    "            else:\n",
    "                captions_dict[image_id] = [caption]\n",
    "    return captions_dict\n",
    "\n",
    "captions = load_captions(caption_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder, Convolutional Neural Network, Pretrained ImageNet\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder, Recurrent Neural Network, LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam search, to transform the Decoder's output into a score for each word in the vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
