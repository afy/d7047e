{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet version 1\n",
    "\n",
    "Hate speech identification project, D7047E <br>\n",
    "Binary text classification task using pretrained XLNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install sentencepiece\n",
    "%pip install transformers\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from transformers import XLNetModel, XLNetConfig, XLNetTokenizer, AutoTokenizer, XLNetForSequenceClassification\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and vars\n",
    "\n",
    "# aux files\n",
    "path_tr = r\"..\\\\OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_te_a = r\"..\\\\OLID_TEST_A_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_te_b = r\"..\\\\OLID_TEST_B_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_te_c = r\"..\\\\OLID_TEST_C_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_temp_vocab_tsv = r\"data\\\\temp_vocab.txt\"\n",
    "path_spm = r\"data\\\\proj_xlnet\" # +.model / +.vocab\n",
    "\n",
    "# Model configs\n",
    "VOCAB_SIZE = 20_000\n",
    "ALWAYS_OVERWRITE_VOCAB = True\n",
    "model_config = XLNetConfig(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    num_labels = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating intermediate vocab help file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13241/13241 [00:00<00:00, 287952.27it/s]\n",
      "100%|██████████| 861/861 [00:00<00:00, 287913.24it/s]\n",
      "100%|██████████| 241/241 [00:00<00:00, 237841.71it/s]\n",
      "100%|██████████| 214/214 [00:00<00:00, 108246.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data, format, preprocess\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "\n",
    "def loadVocabStringFromFile(filepath):\n",
    "    res_s = \"\"\n",
    "    extra = {\"encoding\": \"utf-8\"}\n",
    "    file_len = sum(1 for _ in open(filepath, **extra))\n",
    "    with open(filepath, 'r', **extra) as trf:\n",
    "        for i, line in enumerate(tqdm(trf, total=file_len)):\n",
    "            if i > 0:\n",
    "                l = line.split(\"\\t\")[1:2]\n",
    "                res_s += l[0]+\" \"\n",
    "    return res_s\n",
    "\n",
    "# Load vocab\n",
    "# TEMPORARY NOTE: THIS CODE REQUIRES EMPTY FOLDER [...]/XlNetModel/data/\n",
    "if ALWAYS_OVERWRITE_VOCAB or not os.path.isfile(path_temp_vocab_tsv):\n",
    "    print(\"Creating intermediate vocab help file\")\n",
    "    res_s = \"\" \n",
    "    res_s += loadVocabStringFromFile(path_tr)\n",
    "    res_s += loadVocabStringFromFile(path_te_a)\n",
    "    res_s += loadVocabStringFromFile(path_te_b)\n",
    "    res_s += loadVocabStringFromFile(path_te_c)\n",
    "   \n",
    "    with open(path_temp_vocab_tsv, 'w', encoding=\"utf-8\") as tmpf:\n",
    "        tmpf.write(res_s)\n",
    "else:\n",
    "    print(\"File already found\")\n",
    "\n",
    "spm_tr = spm.SentencePieceTrainer.train(\n",
    "    input = path_tr,\n",
    "    model_prefix=path_spm, \n",
    "    input_format = \"text\",\n",
    "    vocab_size = VOCAB_SIZE\n",
    ")\n",
    "tokenizer = XLNetTokenizer(vocab_file = path_spm+\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13239/13239 [00:00<00:00, 2202808.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('@USER She should ask a few native Americans what their take on this is .',\n",
       " tensor([1]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and split dataset\n",
    "df = pd.read_csv(path_tr, sep=\"\\t\", names=[\"id\",\"text\",\"label\",\"other_1\",\"other_2\"])\n",
    "df = df.drop(axis=0, index=0) # Remove column names\n",
    "df = df.drop(axis=1, labels=[\"id\", \"other_1\", \"other_2\"]).to_dict()\n",
    "for i in trange(len(df[\"label\"])-1):\n",
    "    df[\"label\"][i+1] = 1 if df[\"label\"][i+1] == \"OFF\" else 0\n",
    "\n",
    "def getExample(index):\n",
    "    if index <= 0: index = 1\n",
    "    return df[\"text\"][index], torch.LongTensor([df[\"label\"][index]])\n",
    "\n",
    "getExample(1) # index starts at 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet/xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet/xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "# docs: https://huggingface.co/docs/transformers/model_doc/xlnet\n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet/xlnet-base-cased\", num_labels=2)\n",
    "\n",
    "# Note: cannot get custom configuration to work, maybe worth fixing down the line\n",
    "# either that or review the vocab size. Our vocab is <20k but pretrained model is 32k.\n",
    "#    -> hopefully not a problem (surely)\n",
    "#model = XLNetForSequenceClassification(model_config)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0619, 0.0903]], grad_fn=<AddmmBackward0>)\n",
      "0.6790687441825867\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "ex_input, ex_label = tokenizer(getExample(0)[0], return_tensors=\"pt\"), getExample(0)[1]\n",
    "ex_input['labels'] = ex_label\n",
    "output = model(**ex_input)\n",
    "pred = torch.argmax(output.logits)\n",
    "\n",
    "print(output.logits)\n",
    "print(output.loss.item())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
