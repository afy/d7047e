{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet version 1\n",
    "\n",
    "Hate speech identification project, D7047E <br>\n",
    "Binary text classification task using pretrained XLNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install sentencepiece\n",
    "%pip install transformers\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from transformers import XLNetModel, XLNetConfig, XLNetTokenizer, AutoTokenizer, XLNetForSequenceClassification\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and vars\n",
    "\n",
    "# Note: all below are .tsv format \n",
    "path_tr = r\"..\\\\OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_te_a = r\"..\\\\OLID_TEST_A_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_te_b = r\"..\\\\OLID_TEST_B_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_te_c = r\"..\\\\OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt\" \n",
    "path_temp_vocab_tsv = r\"data\\\\temp_vocab_tsv.txt\"\n",
    "\n",
    "# Model configs\n",
    "GLOBAL_VOCAB_SIZE = 20_000\n",
    "ALWAYS_OVERWRITE_VOCAB = True\n",
    "config = XLNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating intermediate vocab help file\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Internal: D:\\a\\sentencepiece\\sentencepiece\\src\\trainer_interface.cc(329) [trainer_spec_.input_format().empty() || trainer_spec_.input_format() == \"text\" || trainer_spec_.input_format() == \"tsv\"] Supported formats are 'text' and 'tsv'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile already found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m spm_tr \u001b[38;5;241m=\u001b[39m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-sp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_format\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbsp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGLOBAL_VOCAB_SIZE\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hannes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentencepiece\\__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[1;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[1;32m-> 1047\u001b[0m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hannes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentencepiece\\__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[1;34m(arg, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[0;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hannes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentencepiece\\__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceTrainer__TrainFromMap(args)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Internal: D:\\a\\sentencepiece\\sentencepiece\\src\\trainer_interface.cc(329) [trainer_spec_.input_format().empty() || trainer_spec_.input_format() == \"text\" || trainer_spec_.input_format() == \"tsv\"] Supported formats are 'text' and 'tsv'."
     ]
    }
   ],
   "source": [
    "# Load data, format, preprocess\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "\n",
    "# create intermediate \n",
    "\n",
    "if ALWAYS_OVERWRITE_VOCAB or not os.path.isfile(path_temp_vocab_tsv):\n",
    "    print(\"Creating intermediate vocab help file\")\n",
    "    res_s = \"\"\n",
    "    with open(path_tr, 'r') as trf:\n",
    "        for i, line in enumerate(trf.readlines()):\n",
    "            if i > 0:\n",
    "                l = line.split(\"\\t\")[1:2]\n",
    "                res_s += l[0]+\" \"\n",
    "\n",
    "                if i > 10: break\n",
    "    \n",
    "    with open(path_temp_vocab_tsv, 'w') as tmpf:\n",
    "        tmpf.write(res_s)\n",
    "else:\n",
    "    print(\"File already found\")\n",
    "\n",
    "\n",
    "spm_tr = spm.SentencePieceTrainer.train(\n",
    "    input = path_tr,\n",
    "    model_prefix='en-sp', \n",
    "    input_format = \"bsp\",\n",
    "    vocab_size = GLOBAL_VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet/xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet/xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XLNet model\n",
    "# docs: https://huggingface.co/docs/transformers/model_doc/xlnet\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   17, 11368,    19,    94,  2288,    27, 10920,     4,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput'>\n"
     ]
    }
   ],
   "source": [
    "# Run + testing\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "outputs = model(**inputs)\n",
    "print(type(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3253, 1.2618]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
